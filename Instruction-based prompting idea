# ğŸ§  Instruction-Based Prompting for Bulgarian Financial Hierarchy RAG

## ğŸ“„ Source Reference
- **Survey Paper**: [Instruction Tuning for Large Language Models: A Survey (arXiv:2308.10792v8)](https://arxiv.org/abs/2308.10792)
- **Internal Document**: `llama3_instruction_based_rag.md`

## ğŸ” Overview
Instruction-based prompting, also known as instruction tuning (IT), is a supervised fine-tuning strategy where a language model is trained to follow structured (INSTRUCTION, OUTPUT) pairs. This bridges the gap between next-token prediction and human intent.

In the case of Bulgarian financial hierarchy generation via a RAG pipeline using LLaMA 3, instruction-based prompting significantly improves output quality, adherence to format, and control over hallucination.

---

## âœ… Benefits of Instruction-Based Prompting

| Aspect | Benefit |
|--------|---------|
| **Structure Adherence** | Models generate precise symbolic hierarchies with â”œâ”€â”€, â””â”€â”€, etc. |
| **Value Preservation** | Floats like `63.5` are not altered; `NA` is preserved as-is |
| **Hallucination Prevention** | Model is restricted to only what's present in input |
| **Generalization** | With diverse examples, models adapt to varied financial formats |

---

## ğŸ“˜ Strategy From Research
### Paper Highlights (arXiv:2308.10792v8)
- Instruction tuning improves controllability and alignment
- Step-by-step instructions help LLMs follow tasks more consistently
- Instruction format should include:
  - Natural language instruction
  - Input field (optional)
  - Output field (target response)
- Self-instruct and distillation (e.g., Alpaca, WizardLM) improve scale and quality

---

## ğŸ§± Current Instruction Format (Before)

```plaintext
### Instruction:
You are an AI assistant specializing in structuring and organizing data into hierarchical formats...

Step 1: Understand the balance sheet...
Step 2: Establish a perfect tree...
...
```

This structure is already strong but can benefit from more **explicit procedural clarity**.

---

## ğŸ” Proposed: Algorithmic Instruction Format

```markdown
### ğŸ”§ Instruction Head: Symbolic Hierarchy Generation for Bulgarian Financial Assets

You are an AI assistant specializing in structuring Bulgarian financial data into clean hierarchical trees. Follow these steps exactly:

#### âœ… Step 1: Parse the Input
- Input format: `ID | Label | Value1 | Value2`
- Ignore the ID

#### âœ… Step 2: Identify Hierarchy Layers
- Detect categories like Ğ., Ğ‘., Ğ’.
- Recognize nested structures like Roman numerals or financial subcategories

#### âœ… Step 3: Preserve and Format Values
- Keep floats as-is (e.g., `282.75` â†’ `282.75`)
- Keep `NA` unchanged

#### âœ… Step 4: Construct the Tree
- Use symbolic formatting:
  - `â”œâ”€â”€` for siblings
  - `â””â”€â”€` for last sibling
  - `â”‚` for vertical alignment

#### âœ… Step 5: Do Not Hallucinate
- Only include categories from the input
- Do not generate missing or artificial nodes

#### âœ… Step 6: Cross-Verify Output
- Ensure all input labels are present
- Tree should visually and semantically align with financial logic
```

---

## ğŸ“¤ Example Output

```plaintext
ĞĞºÑ‚Ğ¸Ğ²Ğ¸ (NA)
â”œâ”€â”€ Ğ. Ğ—Ğ°Ğ¿Ğ¸ÑĞ°Ğ½, Ğ½Ğ¾ Ğ½ĞµĞ²Ğ½ĞµÑĞµĞ½ ĞºĞ°Ğ¿Ğ¸Ñ‚Ğ°Ğ» (5001.17 | 1240.95)
â”œâ”€â”€ Ğ‘. ĞĞµÑ‚ĞµĞºÑƒÑ‰Ğ¸ (Ğ´ÑŠĞ»Ğ³Ğ¾Ñ‚Ñ€Ğ°Ğ¹Ğ½Ğ¸) Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸ (9981.44 | 5417.92)
â”‚   â”œâ”€â”€ ĞĞµĞ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ½Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸ (3748.61 | 6371.56)
â”‚   â”œâ”€â”€ Ğ”ÑŠĞ»Ğ³Ğ¾Ñ‚Ñ€Ğ°Ğ¹Ğ½Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ½Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸ (3502.83 | 4485.8)
â”‚   â”œâ”€â”€ Ğ”ÑŠĞ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸ (7933.24 | 4934.32)
â”‚   â”œâ”€â”€ ĞÑ‚ÑÑ€Ğ¾Ñ‡ĞµĞ½Ğ¸ Ğ´Ğ°Ğ½ÑŠÑ†Ğ¸ (1359.5 | 6327.22)
â”œâ”€â”€ Ğ’. Ğ¢ĞµĞºÑƒÑ‰Ğ¸ (ĞºÑ€Ğ°Ñ‚ĞºĞ¾Ñ‚Ñ€Ğ°Ğ¹Ğ½Ğ¸) Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸ (2286.89 | 8583.47)
â”‚   â”œâ”€â”€ ĞœĞ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ½Ğ¸ Ğ·Ğ°Ğ¿Ğ°ÑĞ¸ (3681.95 | 6034.81)
â”‚   â”œâ”€â”€ Ğ’Ğ·ĞµĞ¼Ğ°Ğ½Ğ¸Ñ (3248.36 | 7456.5)
â”‚   â”œâ”€â”€ Ğ˜Ğ½Ğ²ĞµÑÑ‚Ğ¸Ñ†Ğ¸Ğ¸ (7350.47 | 6362.39)
â”‚   â”œâ”€â”€ ĞŸĞ°Ñ€Ğ¸Ñ‡Ğ½Ğ¸ ÑÑ€ĞµĞ´ÑÑ‚Ğ²Ğ° (8590.09 | 3104.08)
â”œâ”€â”€ Ğ“. Ğ Ğ°Ğ·Ñ…Ğ¾Ğ´Ğ¸ Ğ·Ğ° Ğ±ÑŠĞ´ĞµÑ‰Ğ¸ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸ (2152.24 | 1409.19)
â”‚   â”œâ”€â”€ Ğ Ğ°Ğ·Ñ…Ğ¾Ğ´Ğ¸ Ğ·Ğ° Ğ½Ğ°ĞµĞ¼Ğ¸ (2226.31 | 4015.26)
â”‚   â”œâ”€â”€ Ğ Ğ°Ğ·Ñ…Ğ¾Ğ´Ğ¸ Ğ·Ğ° Ğ·Ğ°ÑÑ‚Ñ€Ğ°Ñ…Ğ¾Ğ²ĞºĞ¸ (3411.25 | 3739.61)
```

---

## ğŸš€ Next Steps
- Expand with edge case examples (NA-only rows, float rounding errors)
- Add negative examples (e.g., bad trees with hallucinated values)
- Consider formal fine-tuning with SFT dataset (1000+ examples)
- Merge this `.md` into your GitHub repo under `rag-pipeline/instructions/`

--- Beginning phase 1 --- 

ğŸ—‚ï¸ Current Task

Inject this algorithmic instruction into the 1000 symbolic hierarchy training units for Phase 1 fine-tuning.

Output Format:

Each training pair will be structured as:

{
  "instruction": "[Instruction above]",
  "input": "ID | Label | Value1 | Value2\n...",
  "output": "ĞĞºÑ‚Ğ¸Ğ²Ğ¸\nâ”œâ”€â”€ ..."
}

âœ… Once injection is done, we'll proceed to JSONL conversion and training setup on Colab with NLLB-Distilled.
